{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2.6. Intro to TensorFlow and Keras\n",
    "\n",
    "Original code in Nikolenko book was made in **TensorFlow 1.x (TF1)**, but here I will convert examples to **TensorFlow 2.0 (TF2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T18:16:45.149158Z",
     "start_time": "2019-11-17T18:16:45.144034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from models_classes import RegModel\n",
    "from losses import mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Regression using TensorFlow 2\n",
    "\n",
    "$$y = k_{true} * x + b_{true} + \\epsilon$$\n",
    "\n",
    "$$\\epsilon - normal noise (N(0, 2))$$\n",
    "\n",
    "Let's try to recover coefs `k_true`, `b_true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T18:16:47.290872Z",
     "start_time": "2019-11-17T18:16:47.173303Z"
    }
   },
   "outputs": [],
   "source": [
    "# set constants\n",
    "n_samples = 1000\n",
    "batch_size = 100\n",
    "num_steps = 2000\n",
    "display_step = 100\n",
    "\n",
    "k_true = 2\n",
    "b_true = 1\n",
    "\n",
    "# generate data\n",
    "X_data = np.random.uniform(1,10,(n_samples, 1))\n",
    "y_data = k_true * X_data + b_true + np.random.normal(0, 2, (n_samples, 1))\n",
    "\n",
    "# initialize coefs\n",
    "k = tf.Variable(tf.random.normal((1,1), dtype='float64'), name='slope')\n",
    "b = tf.Variable(tf.zeros((1,), dtype='float64'), name='bias')\n",
    "\n",
    "# define train process\n",
    "def train(model, X, y, optimizer):\n",
    "    \"\"\"\n",
    "    One step of train process\n",
    "    \n",
    "    params:\n",
    "        model: model object ot train\n",
    "        X: data\n",
    "        y: target\n",
    "        optimizer: tf optimizer\n",
    "    return:\n",
    "        loss: loss value\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as t:\n",
    "        y_pred = model(X)\n",
    "        loss = mse_loss(y, y_pred)\n",
    "        \n",
    "    grads = t.gradient(loss, [model.k, model.b])\n",
    "    optimizer.apply_gradients(zip(grads,[model.k, model.b]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T18:16:57.573044Z",
     "start_time": "2019-11-17T18:16:55.669092Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: loss=414.14657788073606, k=[[2.07025512]], b=[0.56551404]\n",
      "Epoch 200: loss=385.26826344679296, k=[[2.03992477]], b=[0.69125836]\n",
      "Epoch 300: loss=328.2517944797553, k=[[2.03959199]], b=[0.82006933]\n",
      "Epoch 400: loss=413.48839302558264, k=[[1.99113144]], b=[0.8829619]\n",
      "Epoch 500: loss=382.5397537478649, k=[[2.01100017]], b=[0.93458906]\n",
      "Epoch 600: loss=435.5794218697961, k=[[1.98739557]], b=[1.01688565]\n",
      "Epoch 700: loss=453.8004157614479, k=[[1.98157274]], b=[1.03336836]\n",
      "Epoch 800: loss=348.08688809808166, k=[[2.02365658]], b=[1.05184482]\n",
      "Epoch 900: loss=386.0801130190662, k=[[1.9775763]], b=[1.04739293]\n",
      "Epoch 1000: loss=353.1195900328757, k=[[1.96046547]], b=[1.04601709]\n",
      "Epoch 1100: loss=367.66120716505327, k=[[1.9538338]], b=[1.04458963]\n",
      "Epoch 1200: loss=400.8933636698966, k=[[1.97361014]], b=[1.07146353]\n",
      "Epoch 1300: loss=399.27191244584264, k=[[1.98046705]], b=[1.06778457]\n",
      "Epoch 1400: loss=509.3648788431095, k=[[1.98125481]], b=[1.04335596]\n",
      "Epoch 1500: loss=349.08913021188266, k=[[1.98900598]], b=[1.05509663]\n",
      "Epoch 1600: loss=383.97681930364195, k=[[1.99319279]], b=[1.0713841]\n",
      "Epoch 1700: loss=400.1983631639971, k=[[1.97122276]], b=[1.0531022]\n",
      "Epoch 1800: loss=357.52087278642557, k=[[1.93878802]], b=[1.06192867]\n",
      "Epoch 1900: loss=311.2130701188381, k=[[1.98814925]], b=[1.07522126]\n",
      "Epoch 2000: loss=327.88187518743894, k=[[1.96946669]], b=[1.06341898]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate=1e-4)\n",
    "model = RegModel(k, b)\n",
    "\n",
    "# running optimization\n",
    "for i in range(num_steps):\n",
    "    \n",
    "    # select data batch\n",
    "    indices = np.random.choice(n_samples, batch_size)\n",
    "    X_batch, y_batch = X_data[indices], y_data[indices]\n",
    "    \n",
    "    loss_val = train(model, X_batch, y_batch, optimizer)\n",
    "    \n",
    "    # output info\n",
    "    if (i+1) % display_step == 0:\n",
    "        print(f'Epoch {i+1}: loss={loss_val}, k={model.k.numpy()}, b={model.b.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got approximate values `k = 1.96` and `b = 1.06`.\n",
    "\n",
    "\n",
    "## 2. Logistic Regression using TF2 Keras api\n",
    "\n",
    "- Generate some data;\n",
    "- Define simple logistic regression (1 dense layer with sigmoid activation);\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T16:40:53.393947Z",
     "start_time": "2019-11-17T16:40:53.388485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2) (2000, 1)\n",
      "(200, 2) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "def sampler(n, x, y):\n",
    "    return np.random.normal(size=[n,2]) + [x, y]\n",
    "\n",
    "def sample_data(n=1000, p0=(-1., -1.), p1=(1., 1.)):\n",
    "    zeros, ones = np.zeros((n, 1)), np.ones((n, 1))\n",
    "    labels = np.vstack([zeros, ones])\n",
    "    \n",
    "    z_sample = sampler(n, x=p0[0], y=p0[1])\n",
    "    o_sample = sampler(n, x=p1[0], y=p1[1])\n",
    "    \n",
    "    return np.vstack([z_sample, o_sample]), labels\n",
    "\n",
    "X_train, Y_train = sample_data()\n",
    "X_test, Y_test = sample_data(100)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T16:40:55.027521Z",
     "start_time": "2019-11-17T16:40:55.016931Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = tf.keras.Sequential(\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    ")\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T16:41:33.988848Z",
     "start_time": "2019-11-17T16:41:31.339529Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 200 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 0s 45us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.2009 - accuracy: 0.9190 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 0s 46us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 0s 47us/sample - loss: 0.2009 - accuracy: 0.9190 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.2009 - accuracy: 0.9190 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9180 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9190 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 0s 43us/sample - loss: 0.2009 - accuracy: 0.9185 - val_loss: 0.1874 - val_accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18bc3bf9b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(X_train, Y_train, epochs=30, batch_size=16, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got `train accuracy = 0.9185` and `validation accuracy = 0.9100`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
